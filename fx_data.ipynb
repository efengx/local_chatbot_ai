{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：下载文档\n",
    "file=\"https://arxiv.org/pdf/2307.09288.pdf\"\n",
    "!mkdir -p data && wget --user-agent \"Mozilla\" {file} -O \"./data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：下载文档\n",
    "file=\"https://ir.manutd.com/~/media/Files/M/Manutd-IR/documents/manu-20f-2022-09-24.pdf\"\n",
    "!mkdir -p data && wget --user-agent \"Mozilla\" {file} -P \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：下载模型，量化模型q4\n",
    "# 参考：（size: 7.73G）https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/tree/main\n",
    "# 说明：local_dir 使用的是软链接\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "                filename=\"llama-2-13b-chat.Q4_0.gguf\",\n",
    "                local_dir=\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：下载模型，量化模型q4\n",
    "# 说明：该模型llamacpp无法加载，存在错误\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"soulteary/Chinese-Llama-2-7b-ggml-q4\",\n",
    "                filename=\"Chinese-Llama-2-7b-ggml-q4.bin\",\n",
    "                local_dir=\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：（本地微服务）嵌入式milvus\n",
    "# 启动时间：1m19.3s\n",
    "from src.local_milvus import LocalMilvus\n",
    "\n",
    "localMilvus = LocalMilvus.instance()\n",
    "localMilvus.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：关闭嵌入式milvus\n",
    "from src.local_milvus import LocalMilvus\n",
    "\n",
    "localMilvus = LocalMilvus.instance()\n",
    "localMilvus.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：启动milvus调试模式\n",
    "# 参考：https://github.com/milvus-io/milvus-lite\n",
    "!milvus-server --data \"./db/milvus_data\" --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：milvus api\n",
    "from pymilvus import connections, utility, Collection, db\n",
    "\n",
    "print(\"==list_connections:\", connections.list_connections())\n",
    "alias = connections.list_connections()[0][0]\n",
    "print(alias)\n",
    "\n",
    "connections.connect(\n",
    "  \"default\",\n",
    "  host='39.104.228.125',\n",
    "  port='19530'\n",
    ")\n",
    "\n",
    "# 查询数据库，集群最多支持64个数据库。\n",
    "print(\"==list_database:\", db.list_database())\n",
    "\n",
    "print(\"==get_server_version:\", utility.get_server_version())\n",
    "\n",
    "# 使用指定的数据库，如果未指定，则collection默认使用default数据库\n",
    "# db.using_database(\"default\")\n",
    "\n",
    "# 获取default数据库中的集合，一个集合由多个分区组成（表），会创建一个默认的分区_default\n",
    "# 集合可以放在多个分片（不同的硬件节点，提升多写能力）上\n",
    "print(\"==list_collections:\", utility.list_collections())\n",
    "\n",
    "print(\"==has_collection:\", utility.has_collection(\"gptcache\"))\n",
    "\n",
    "collection = Collection(\"gptcache\")\n",
    "print(\"==schema\", collection.schema)\n",
    "print(\"==description\", collection.description)\n",
    "print(\"==name\", collection.name)\n",
    "print(\"==is_empty\", collection.is_empty)\n",
    "print(\"==num_entities\", collection.num_entities)\n",
    "print(\"==primary_field\", collection.primary_field)\n",
    "print(\"==partitions\", collection.partitions)\n",
    "print(\"==indexes\", collection.indexes)\n",
    "# print(\"==properties\", collection.properties)\n",
    "\n",
    "# 删除集合\n",
    "# utility.drop_collection(\"gptcache\")\n",
    "\n",
    "\n",
    "# 查看索引构建进度（向量索引，标量索引）\n",
    "print(\"==index_building_progress\", utility.index_building_progress(\"gptcache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：删除指定集合\n",
    "utility.drop_collection(\"LangChainCollection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 任务：sqlit\n",
    "from sqlalchemy import create_engine, inspect, func, select\n",
    "from gptcache.manager import CacheBase\n",
    "from sqlalchemy.ext.serializer import loads, dumps\n",
    "from sqlalchemy.orm import load_only, aliased\n",
    "\n",
    "# 创建数据库链接，包含链接池\n",
    "cacheBase = CacheBase('sqlite', sql_url=\"sqlite:///./db/sqlite.db\")\n",
    "\n",
    "ins = inspect(cacheBase._engine)\n",
    "# 查询数据库名\n",
    "print(ins.get_schema_names())\n",
    "# 获取表名\n",
    "print(ins.get_table_names())\n",
    "# 获取表的字段名\n",
    "print(ins.get_columns('gptcache_question'))\n",
    "\n",
    "# 获取session\n",
    "session = cacheBase.Session()\n",
    "\n",
    "# 查看指定表的长度\n",
    "count = session.query(func.count(cacheBase._ques.id)).scalar()\n",
    "print(\"count=\", count)\n",
    "\n",
    "# 查询cacheBase._ques表\n",
    "# list_item = session.query(cacheBase._ques).all()\n",
    "# for item in list_item:\n",
    "#     print(item.__dict__)\n",
    "\n",
    "#  分页查询\n",
    "# list_item = session.scalars(select(cacheBase._ques).limit(1).offset(0))\n",
    "# for item in list_item:\n",
    "#     print(item.__dict__)\n",
    "\n",
    "def as_dict(obj):\n",
    "       print(obj)\n",
    "       print(type(obj))\n",
    "       # print(isinstance(obj, ))\n",
    "       print(obj.__table__)\n",
    "       print(obj.__table__.columns)\n",
    "       return {c.name: str(getattr(obj, c.name)) for c in obj.__table__.columns}\n",
    "\n",
    "# list_item = [as_dict(item) for item in session.scalars(select(cacheBase._ques).limit(1).offset(0))]\n",
    "# print(list_item)\n",
    "\n",
    "# 分页查询 join\n",
    "# list_item = [as_dict(item) for item in session.scalars(\n",
    "#        select(cacheBase._answer).join(cacheBase._ques, cacheBase._answer.question_id == cacheBase._ques.id).limit(1).offset(0))]\n",
    "# print(list_item)\n",
    "\n",
    "# id =  aliased(cacheBase._answer.id, name=\"id\")\n",
    "\n",
    "list_item = [item._mapping for item in session.query(\n",
    "       cacheBase._answer.id, cacheBase._answer.answer, cacheBase._ques.question).filter(\n",
    "              cacheBase._answer.question_id == cacheBase._ques.id).limit(1).offset(0)]\n",
    "print(list_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：加载外部数据\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务：RAG\n",
    "import os\n",
    "from langchain.chains import StuffDocumentsChain, LLMChain\n",
    "from gptcache.embedding import Huggingface\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import Milvus\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# 默认提示\n",
    "stuff_prompt_override = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "llm = OpenAI()\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 覆盖提示\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    ")\n",
    "\n",
    "\n",
    "# 需要查询的问题\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "# 嵌入的文本\n",
    "texts = [\n",
    "    \"Basquetball is a great sport.\",\n",
    "    \"Fly me to the moon is one of my favourite songs.\",\n",
    "    \"The Celtics are my favourite team.\",\n",
    "    \"This is a document about the Boston Celtics\",\n",
    "    \"I simply love going to the movies\",\n",
    "    \"The Boston Celtics won the game by 20 points\",\n",
    "    \"This is just a random text.\",\n",
    "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "    \"L. Kornet is one of the best Celtics players.\",\n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "]\n",
    "\n",
    "# 创建一个检索器\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "\n",
    "# 测试\n",
    "# emb = embeddings.client.encode(\"你好，世界!\")\n",
    "# print(\"emb1=\", emb)\n",
    "\n",
    "# emb2 = Huggingface(model=\"BAAI/bge-small-zh-v1.5\").to_embeddings(\"你好，世界!\")\n",
    "# print(\"emb2=\", emb2)\n",
    "os.environ[\"MILVUS_HOST\"] = os.getenv('MILVUS_HOST')\n",
    "retriever = Milvus.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(\"docs=\", docs)\n",
    "\n",
    "# 重新对文档进行排序，相关性较高的位于文档的开头和结尾，相关性较低的位于中间\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "print(\"reordered_docs=\", reordered_docs)\n",
    "\n",
    "result = chain.run(input_documents=reordered_docs, query=query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
